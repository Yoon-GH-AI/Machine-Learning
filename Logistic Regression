##$$s(z) = \frac{1}{1+e^{-z}} $$
def sigmoid(z):
    return 1.0/(1 + np.exp(-z))

##Logistic Regression's Loss function differs from Linear Regereesion's Loss fuction
##$$ Logistic Regression's Loss function: $$
##$$ J_{BCE} = -\frac{1}{N}Σ_i[y_i*log_ep_i + (1-y_i)*log_e(1-p_i)] $$


##$$ Logistic Regression's Loss function: $$
##$$ J_{BCE} = -\frac{1}{N}Σ[y*log_e\hat y + (1-y)*log_e(1-\hat y)]$$
def loss(y, y_hat):
    loss = -np.mean(y*(np.log(y_hat)) - (1-y)*np.log(1-y_hat))
    return loss

##> Question: *Given p = [0.7, 0.3, 0.4] and their actual label y = [1, 0, 0], what is the BCE (Binary Cross-Entropy Loss)?*
##> By hand
##$$ J_{BCE} = -\frac{1}{3}[1*log_e0.7+(1-1)*log_e(1-0.7)+0*log_e0.3+(1-0)*log_e(1-0.3)+0*log_e0.4+(1-0)*log_e(1-0.4)] $$
##$$ = -\frac{1}{3}[1*log_e0.7+0*log_e0.3+0*log_e0.3+1*log_e0.7+0*log_e0.4+1*log_e0.6] $$
##$$ = -\frac{1}{3}[1*log_e0.7+1*log_e0.7+1*log_e0.6] $$
##$$ = 0.408 $$

import numpy as np

def loss(y, p):
    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))
    return loss

# Convert lists to numpy arrays
y = np.array([1, 0, 0])
p = np.array([0.7, 0.3, 0.4])

# Compute loss
bce_loss = loss(y, p)
print(f"BCE Loss: {bce_loss:.3f}")  # Output: BCE Loss: 0.4081

import numpy as np
from sklearn.datasets import make_classification

# Creat Input and true/target value
X, y = make_classification(
    n_features=2,      # Total features = 2 (e.g., x1, x2)
    n_redundant=0,     # No redundant features
    n_informative=2,   # Both features are informative
    random_state=1,    # Fixes randomness for reproducibility
    n_clusters_per_class=1  # Each class has 1 cluster (simpler separation)
)

def sigmoid(z):
    return 1.0/(1 + np.exp(-z))

def loss(y, p):
    loss = -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))
    return loss

def accuracy(y, p):
    accuracy = np.sum(y == p) / len(y)
    return accuracy

def gradients(X, y, p):

    # X --> Input.
    # y --> true/target value.
    # p --> hypothesis/predictions.
    # w --> weights (parameter).
    # b --> bias (parameter).

    # m-> number of training examples.
    m = X.shape[0]

    # Gradient of loss w.r.t weights.
    dw = (1/m)*np.dot(X.T, (p - y))
    # X "numpy matrix" data: X @ (y_hat - y)

    # Gradient of loss w.r.t bias.
    db = (1/m)*np.sum((p - y))


    return dw, db

def normalize(X):

    # X --> Input.

    # m-> number of training examples
    # n-> number of features
    m, n = X.shape

    # Normalizing all the n features of X.
    for i in range(n):
        X = (X - X.mean(axis=0))/X.std(axis=0)

    return X

def predict(X):

    # X --> Input.

    # Normalizing the inputs.
    x = normalize(X)

    # Calculating presictions/p.
    preds = sigmoid(np.dot(x, w) + b)

    # Empty List to store predictions.
    pred_class = []
    # if p >= 0.5 --> round up to 1
    # if p < 0.5 --> round up to 1
    pred_class = [1 if i > 0.5 else 0 for i in preds]

    return np.array(pred_class)

def train(X, y, bs, epochs, lr):

    # X --> Input.
    # y --> true/target value.
    # bs --> Batch Size.
    # epochs --> Number of iterations.
    # lr --> Learning rate.

    # m-> number of training examples
    # n-> number of features
    m, n = X.shape

    # Initializing weights and bias to zeros.
    w = np.ones((n,1))
    b = 1

    # Reshaping y.
    y = y.reshape(m,1)

    # Normalizing the inputs.
    x = normalize(X)

    # Empty list to store losses.
    losses = []

    # Training loop.
    for epoch in range(epochs):
        for i in range((m-1)//bs + 1):

            # Defining batches. SGD.
            start_i = i*bs
            end_i = start_i + bs
            xb = x[start_i:end_i]
            yb = y[start_i:end_i]

            # Calculating hypothesis/prediction.
            p = sigmoid(np.dot(xb, w) + b)
            # -z = np.dot(xb, w) + b
            # 1.0/(1 + np.exp(-(np.dot(xb, w) + b)))
            # def sigmoid(z):
                # return 1.0/(1 + np.exp(-z))


            # Getting the gradients of loss w.r.t parameters.
            dw, db = gradients(xb, yb, p)

            # Updating the parameters.
            w -= lr*dw
            b -= lr*db

        # Calculating loss and appending it in the list.
        l = loss(y, sigmoid(np.dot(X, w) + b))
        losses.append(l)

    # returning weights, bias and losses(List).
    return w, b, losses

# Training
w, b, l = train(X, y, bs=10, epochs=100, lr=0.01)

accuracy(y, predict(X))
